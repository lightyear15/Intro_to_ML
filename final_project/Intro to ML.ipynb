{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Udacity Data Analyst Nanodegree, Project 5"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Intro to Machine Learning"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Final Project by Giulio Ministeri"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Dataset Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal of this project is to create a predictor capable of detecting people who were involved in some way in the Enron fraud, based principally on the analysis of their financial data and sent and received e-mails. Machine learning algorithms come in handy in these cases, where there are no specific features or ease-to-detect patterns in the data such that one can simply checks for these evidences and reliably predicts the outcomes. Heuristics solutions, i.e. long list of \u201cif...then...else\u201d, are unfeasible, while machine learning algorithms learn, reshape and rework input features for us giving back a \u201cbox\u201d ables to predict the output if fed with new inputs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import pickle\n",
      "sys.path.append(\"../tools/\")\n",
      "\n",
      "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
      "print len(data_dict)\n",
      "print data_dict.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "146\n",
        "['METTS MARK', 'BAXTER JOHN C', 'ELLIOTT STEVEN', 'CORDES WILLIAM R', 'HANNON KEVIN P', 'MORDAUNT KRISTINA M', 'MEYER ROCKFORD G', 'MCMAHON JEFFREY', 'HORTON STANLEY C', 'PIPER GREGORY F', 'HUMPHREY GENE E', 'UMANOFF ADAM S', 'BLACHMAN JEREMY M', 'SUNDE MARTIN', 'GIBBS DANA R', 'LOWRY CHARLES P', 'COLWELL WESLEY', 'MULLER MARK S', 'JACKSON CHARLENE R', 'WESTFAHL RICHARD K', 'WALTERS GARETH W', 'WALLS JR ROBERT H', 'KITCHEN LOUISE', 'CHAN RONNIE', 'BELFER ROBERT', 'SHANKMAN JEFFREY A', 'WODRASKA JOHN', 'BERGSIEKER RICHARD P', 'URQUHART JOHN A', 'BIBI PHILIPPE A', 'RIEKER PAULA H', 'WHALEY DAVID A', 'BECK SALLY W', 'HAUG DAVID L', 'ECHOLS JOHN B', 'MENDELSOHN JOHN', 'HICKERSON GARY J', 'CLINE KENNETH W', 'LEWIS RICHARD', 'HAYES ROBERT E', 'MCCARTY DANNY J', 'KOPPER MICHAEL J', 'LEFF DANIEL P', 'LAVORATO JOHN J', 'BERBERIAN DAVID', 'DETMERING TIMOTHY J', 'WAKEHAM JOHN', 'POWERS WILLIAM', 'GOLD JOSEPH', 'BANNANTINE JAMES M', 'DUNCAN JOHN H', 'SHAPIRO RICHARD S', 'SHERRIFF JOHN R', 'SHELBY REX', 'LEMAISTRE CHARLES', 'DEFFNER JOSEPH M', 'KISHKILL JOSEPH G', 'WHALLEY LAWRENCE G', 'MCCONNELL MICHAEL S', 'PIRO JIM', 'DELAINEY DAVID W', 'SULLIVAN-SHAKLOVITZ COLLEEN', 'WROBEL BRUCE', 'LINDHOLM TOD A', 'MEYER JEROME J', 'LAY KENNETH L', 'BUTTS ROBERT H', 'OLSON CINDY K', 'MCDONALD REBECCA', 'CUMBERLAND MICHAEL S', 'GAHN ROBERT S', 'MCCLELLAN GEORGE', 'HERMANN ROBERT J', 'SCRIMSHAW MATTHEW', 'GATHMANN WILLIAM D', 'HAEDICKE MARK E', 'BOWEN JR RAYMOND M', 'GILLIS JOHN', 'FITZGERALD JAY L', 'MORAN MICHAEL P', 'REDMOND BRIAN L', 'BAZELIDES PHILIP J', 'BELDEN TIMOTHY N', 'DURAN WILLIAM D', 'THORN TERENCE H', 'FASTOW ANDREW S', 'FOY JOE', 'CALGER CHRISTOPHER F', 'RICE KENNETH D', 'KAMINSKI WINCENTY J', 'LOCKHART EUGENE E', 'COX DAVID', 'OVERDYKE JR JERE C', 'PEREIRA PAULO V. FERRAZ', 'STABLER FRANK', 'SKILLING JEFFREY K', 'BLAKE JR. NORMAN P', 'SHERRICK JEFFREY B', 'PRENTICE JAMES', 'GRAY RODNEY', 'PICKERING MARK R', 'THE TRAVEL AGENCY IN THE PARK', 'NOLES JAMES L', 'KEAN STEVEN J', 'TOTAL', 'FOWLER PEGGY', 'WASAFF GEORGE', 'WHITE JR THOMAS E', 'CHRISTODOULOU DIOMEDES', 'ALLEN PHILLIP K', 'SHARP VICTORIA T', 'JAEDICKE ROBERT', 'WINOKUR JR. HERBERT S', 'BROWN MICHAEL', 'BADUM JAMES P', 'HUGHES JAMES A', 'REYNOLDS LAWRENCE', 'DIMICHELE RICHARD G', 'BHATNAGAR SANJAY', 'CARTER REBECCA C', 'BUCHANAN HAROLD G', 'YEAP SOON', 'MURRAY JULIA H', 'GARLAND C KEVIN', 'DODSON KEITH', 'YEAGER F SCOTT', 'HIRKO JOSEPH', 'DIETRICH JANET R', 'DERRICK JR. JAMES V', 'FREVERT MARK A', 'PAI LOU L', 'BAY FRANKLIN R', 'HAYSLETT RODERICK J', 'FUGH JOHN L', 'FALLON JAMES B', 'KOENIG MARK E', 'SAVAGE FRANK', 'IZZO LAWRENCE L', 'TILNEY ELIZABETH A', 'MARTIN AMANDA K', 'BUY RICHARD B', 'GRAMM WENDY L', 'CAUSEY RICHARD A', 'TAYLOR MITCHELL S', 'DONAHUE JR JEFFREY M', 'GLISAN JR BEN F']\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The dataset has 146 entries, but 2 of them are named \"THE TRAVEL AGENCY IN THE PARK\" and \"TOTAL\" which of course must be removed"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "del data_dict[\"THE TRAVEL AGENCY IN THE PARK\"]\n",
      "del data_dict[\"TOTAL\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print data_dict[\"METTS MARK\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'salary': 365788, 'to_messages': 807, 'deferral_payments': 'NaN', 'total_payments': 1061827, 'exercised_stock_options': 'NaN', 'bonus': 600000, 'restricted_stock': 585062, 'shared_receipt_with_poi': 702, 'restricted_stock_deferred': 'NaN', 'total_stock_value': 585062, 'expenses': 94299, 'loan_advances': 'NaN', 'from_messages': 29, 'other': 1740, 'from_this_person_to_poi': 1, 'poi': False, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 'NaN', 'email_address': 'mark.metts@enron.com', 'from_poi_to_this_person': 38}\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The financial data includes [\"salary\", \"deferral_payments\", \"total_payments\", \"loan_advances\", \"bonus\", \"restricted_stock_deferred\", \"deferred_income\", \"total_stock_value\", \"expenses\", \"exercised_stock_options\", \"other\", \"long_term_incentive\", \"restricted_stock\", \"director_fees\"]; while for the mail data, a small set of summarizing features are provided : [\"to_messages\", \"email_address\", \"from_poi_to_this_person\", \"from_messages\", \"from_this_person_to_poi\", \"shared_receipt_with_poi\"].\n",
      "The response variable is named \"poi\", it is a boolean variable which identifies people of interest.\n",
      "As can be noted also in the \"METTS MARK\" data, there are a lot of missing data marked as \"NaN\".\n",
      "Before starting analyzing data I move the dataset to a panda Dataframe"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "data_frame = pd.DataFrame(index=data_dict.keys(), columns=data_dict[\"METTS MARK\"].keys())\n",
      "for name in data_dict.keys():\n",
      "    data_frame.loc[name] = pd.Series(data_dict[name])\n",
      "data_frame.replace(\"NaN\", np.nan, inplace=True)\n",
      "data_frame[\"email_address\"].replace(np.nan, \"\", inplace=True)\n",
      "data_frame.index.name = \"full_name\"\n",
      "data_frame = data_frame.reset_index()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As first step I count the number of poi and non-poi to get an idea of how the dataset is distributed among the two classes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"non-POIs people count: \" + str(data_frame[\"poi\"].loc[data_frame[\"poi\"]==False].count())\n",
      "print \"POIs people count: \" + str(data_frame[\"poi\"].loc[data_frame[\"poi\"]==True].count())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "non-POIs people count: 126\n",
        "POIs people count: 18\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Feature Analysis and outliers detection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then I count the number of NaN for each column and for each row. In this way I check if there are some entries or some features that should be dropped due to too many missing data. I also decide to discard rows or columns which have less than 25 % values consistent"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "thre = 0.30\n",
      "person_classes = { \"lessthre\":0, \"less2thre\": 0, \"less3thre\": 0, \"more3thre\":0}\n",
      "for person_index in data_frame.index:\n",
      "    person = data_frame.iloc[person_index]    \n",
      "    ratio = float(person.count()) / float(len(person))\n",
      "    #print person[\"full_name\"] + \" ratio: \" + str(ratio)\n",
      "    if ratio < thre:\n",
      "        person_classes[\"lessthre\"] += 1\n",
      "    elif ratio < 2*thre:\n",
      "        person_classes[\"less2thre\"] += 1\n",
      "    elif ratio < 3*thre:\n",
      "        person_classes[\"less3thre\"] += 1\n",
      "    else:\n",
      "        person_classes[\"more3thre\"] += 1\n",
      "\n",
      "print \"employees count: \"\n",
      "print person_classes\n",
      "\n",
      "toRemove = []\n",
      "for person_index in data_frame.index:    \n",
      "    person = data_frame.iloc[person_index]    \n",
      "    ratio = float(person.count()) / float(len(person))\n",
      "    if ratio < thre:\n",
      "        toRemove.append(person_index)\n",
      "        print \"removing \" + person[\"full_name\"] + \" who is poi: \" + str (person[\"poi\"])\n",
      "    \n",
      "data_frame.drop(toRemove, axis= 0, inplace = True)\n",
      "        \n",
      "feature_classes = { \"lessthre\":0, \"less2thre\": 0, \"less3thre\": 0, \"more3thre\":0}\n",
      "for feature in data_frame.columns:    \n",
      "    ratio = float(data_frame[feature].count()) / float(len(data_frame[feature]))    \n",
      "    if ratio < thre:\n",
      "        feature_classes[\"lessthre\"] += 1\n",
      "    elif ratio < 2*thre:\n",
      "        feature_classes[\"less2thre\"] += 1\n",
      "    elif ratio < 3*thre:\n",
      "        feature_classes[\"less3thre\"] += 1\n",
      "    else:\n",
      "        feature_classes[\"more3thre\"] += 1\n",
      "print \"features count\"\n",
      "print feature_classes\n",
      "\n",
      "toRemove = []\n",
      "for feature in data_frame.columns:    \n",
      "    ratio = float(data_frame[feature].count()) / float(len(data_frame[feature]))\n",
      "    if ratio < thre:\n",
      "        toRemove.append(feature)\n",
      "        print \"removing \" + feature\n",
      "    \n",
      "data_frame.drop(toRemove, axis= 1, inplace = True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "employees count: \n",
        "{'lessthre': 11, 'less2thre': 64, 'less3thre': 65, 'more3thre': 4}\n",
        "removing WODRASKA JOHN who is poi: False\n",
        "removing WHALEY DAVID A who is poi: False\n",
        "removing CLINE KENNETH W who is poi: False\n",
        "removing WAKEHAM JOHN who is poi: False\n",
        "removing WROBEL BRUCE who is poi: False\n",
        "removing SCRIMSHAW MATTHEW who is poi: False\n",
        "removing GILLIS JOHN who is poi: False"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "removing LOCKHART EUGENE E who is poi: False\n",
        "removing CHRISTODOULOU DIOMEDES who is poi: False\n",
        "removing SAVAGE FRANK who is poi: False\n",
        "removing GRAMM WENDY L who is poi: False\n",
        "features count\n",
        "{'lessthre': 4, 'less2thre': 2, 'less3thre': 13, 'more3thre': 3}\n",
        "removing deferral_payments\n",
        "removing restricted_stock_deferred\n",
        "removing loan_advances\n",
        "removing director_fees\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As second step I analyze data to find outliers. Since the dataset is not so big and pois are a small fraction of the total, for now I just count them.\n",
      "For detection I split the dataset in finance and mail subset, leaving out the latter. I also discard \"poi\" and \"email_address\" features. I use linear regression and for X axis I pick \"total_payments\" as base feature."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "finance_feature = [\"salary\", \"bonus\", \"total_stock_value\", \"expenses\", \n",
      "                   \"exercised_stock_options\", \"other\", \"restricted_stock\"]\n",
      "finance_base_feature = \"total_payments\"\n",
      "outlier_list = {}\n",
      "\n",
      "from sklearn import linear_model\n",
      "from scipy import stats\n",
      "\n",
      "\n",
      "for f in finance_feature:\n",
      "    tmp_frame = data_frame.loc[~((data_frame[finance_base_feature].isnull()) | (data_frame[f].isnull()))]\n",
      "    tmp_frame.reset_index(inplace=True)\n",
      "    X = np.reshape( np.array(tmp_frame[finance_base_feature]), (len(tmp_frame[finance_base_feature]), 1))\n",
      "    Y = np.reshape( np.array(tmp_frame[f]), (len(tmp_frame[f]), 1))\n",
      "    reg = linear_model.LinearRegression()\n",
      "    reg.fit (X, Y)\n",
      "    pred = reg.predict(X)\n",
      "    error = abs(pred - Y)\n",
      "    threshold = stats.scoreatpercentile(error, 90)\n",
      "    for i in range(0,len(error)):\n",
      "        if error[i] > threshold:\n",
      "            name = tmp_frame[\"full_name\"].loc[i]\n",
      "            if name in outlier_list.keys():\n",
      "                outlier_list[name] += 1\n",
      "            else:\n",
      "                outlier_list[name] = 1            \n",
      "for i in range(0, len(finance_feature)+1):\n",
      "    cc = sum(np.array(outlier_list.values()) == i)\n",
      "    print \"outliers on \" + str(i) + \" features: \" + str(cc)\n",
      "print \"total outliers: \" + str(len(outlier_list.keys()))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "outliers on 0 features: 0\n",
        "outliers on 1 features: 22\n",
        "outliers on 2 features: 9\n",
        "outliers on 3 features: 6\n",
        "outliers on 4 features: 1\n",
        "outliers on 5 features: 1\n",
        "outliers on 6 features: 0\n",
        "outliers on 7 features: 0\n",
        "total outliers: 39\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The complete list of all the outliers counts 39 names; almost 1/4 of the dataset. I decide to remove only the entries which are at least outliers for 4 features. I end up removing 4 names:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''''\n",
      "for name in outlier_list.keys():\n",
      "    if outlier_list[name] >= 4:\n",
      "        person = data_frame.loc[data_frame[\"full_name\"] == name]\n",
      "        print \"removing: \" + name + \" who is poi: \" + str(person[\"poi\"])\n",
      "        data_frame = data_frame.loc[data_frame[\"full_name\"] != name]\n",
      "        ''''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "removing: SKILLING JEFFREY K who is poi: 95    True\n",
        "Name: poi, dtype: bool\n",
        "removing: PAI LOU L who is poi: 128    False\n",
        "Name: poi, dtype: bool\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As next step I decide to quit the procedure for feature selection for finance feature since I ended up keeping only 8 features.\n",
      "Now I start to analyze email data. My intention is to create a bag of words for mails sent or received to pois and non-pois. But before start doing this I have to check if the mails distribution makes it possible."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "email_feature_list = [\"to_messages\", \"email_address\", \"from_poi_to_this_person\", \"from_messages\", \n",
      "                      \"from_this_person_to_poi\", \"shared_receipt_with_poi\"]\n",
      "email_data_frame = data_frame[email_feature_list]\n",
      "print email_data_frame.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "        to_messages  from_poi_to_this_person  from_messages  \\\n",
        "count     85.000000                85.000000      85.000000   \n",
        "mean    2055.588235                64.623529     614.682353   \n",
        "std     2592.430944                87.458695    1851.144332   \n",
        "min       57.000000                 0.000000      12.000000   \n",
        "25%      533.000000                10.000000      22.000000   \n",
        "50%     1184.000000                35.000000      41.000000   \n",
        "75%     2598.000000                67.000000     146.000000   \n",
        "max    15149.000000               528.000000   14368.000000   \n",
        "\n",
        "       from_this_person_to_poi  shared_receipt_with_poi  \n",
        "count                85.000000                85.000000  \n",
        "mean                 41.364706              1166.282353  \n",
        "std                 100.659474              1181.498218  \n",
        "min                   0.000000                 2.000000  \n",
        "25%                   1.000000               233.000000  \n",
        "50%                   8.000000               739.000000  \n",
        "75%                  24.000000              1847.000000  \n",
        "max                 609.000000              5521.000000  \n",
        "\n",
        "[8 rows x 5 columns]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/lib/python2.7/dist-packages/pandas/compat/scipy.py:68: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
        "  score = values[idx]\n",
        "/usr/lib/python2.7/dist-packages/pandas/compat/scipy.py:68: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
        "  score = values[idx]\n",
        "/usr/lib/python2.7/dist-packages/pandas/compat/scipy.py:68: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
        "  score = values[idx]\n",
        "/usr/lib/python2.7/dist-packages/pandas/compat/scipy.py:68: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
        "  score = values[idx]\n",
        "/usr/lib/python2.7/dist-packages/pandas/compat/scipy.py:68: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
        "  score = values[idx]\n",
        "/usr/lib/python2.7/dist-packages/pandas/compat/scipy.py:68: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
        "  score = values[idx]\n",
        "/usr/lib/python2.7/dist-packages/pandas/compat/scipy.py:68: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
        "  score = values[idx]\n",
        "/usr/lib/python2.7/dist-packages/pandas/compat/scipy.py:68: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
        "  score = values[idx]\n",
        "/usr/lib/python2.7/dist-packages/pandas/compat/scipy.py:68: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
        "  score = values[idx]\n",
        "/usr/lib/python2.7/dist-packages/pandas/compat/scipy.py:68: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
        "  score = values[idx]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are a lot of variability on emails data, \"from messages\" for example ranges from 12 to 14 thousands. Probably there are outliers also in this emails-related feature subset.\n",
      "Second, as a form of feature scaling, I think that features like \"from_poi_to_this_person\", \"from_this_person_to_poi\" and \"shared_receipt_with_poi\" should be scaled by the total count of sent and received emails respectively."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "email_feature_list = ((\"from_messages\", [\"from_this_person_to_poi\"]), \n",
      "                     ((\"to_messages\"), [\"from_poi_to_this_person\", \"shared_receipt_with_poi\"]))\n",
      "outlier_list = {}\n",
      "for feature_set in email_feature_list:\n",
      "    base_f = feature_set[0]\n",
      "    for f in feature_set[1]:        \n",
      "        tmp_frame = data_frame.loc[~((data_frame[base_f].isnull()) | (data_frame[f].isnull()))]\n",
      "        tmp_frame.reset_index(inplace=True)\n",
      "        X = np.reshape( np.array(tmp_frame[base_f]), (len(tmp_frame[base_f]), 1))\n",
      "        Y = np.reshape( np.array(tmp_frame[f]), (len(tmp_frame[f]), 1))\n",
      "        reg = linear_model.LinearRegression()\n",
      "        reg.fit (X, Y)\n",
      "        pred = reg.predict(X)\n",
      "        error = abs(pred - Y)\n",
      "        threshold = stats.scoreatpercentile(error, 90)\n",
      "        for i in range(0,len(error)):\n",
      "            if error[i] > threshold:\n",
      "                name = tmp_frame[\"full_name\"].loc[i]\n",
      "                if name in outlier_list.keys():\n",
      "                    outlier_list[name] += 1\n",
      "                else:\n",
      "                    outlier_list[name] = 1            \n",
      "for i in range(0, 4):\n",
      "    cc = sum(np.array(outlier_list.values()) == i)        \n",
      "    print \"outliers on \" + str(i) + \" features: \" + str(cc)\n",
      "print \"total outliers: \" + str(len(outlier_list.keys()))\n",
      "print \"total dataset length: \" + str(len(data_frame))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "outliers on 0 features: 0\n",
        "outliers on 1 features: 14\n",
        "outliers on 2 features: 5\n",
        "outliers on 3 features: 1\n",
        "total outliers: 20\n",
        "total dataset length: 131\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I remove outliers for more than 2 features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for name in outlier_list.keys():\n",
      "    if outlier_list[name] >= 2:\n",
      "        print \"removing: \" + name\n",
      "        data_frame = data_frame.loc[data_frame[\"full_name\"] != name]\n",
      "print \"final dataset length: \" + str(len(data_frame))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "removing: KEAN STEVEN J\n",
        "removing: SHAPIRO RICHARD S\n",
        "removing: BELDEN TIMOTHY N\n",
        "removing: LAVORATO JOHN J\n",
        "removing: KAMINSKI WINCENTY J\n",
        "removing: FREVERT MARK A\n",
        "final dataset length: 125\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Feature Selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For feature selection I use tree-base feature selection. I substitute NaN values with zeros"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import ensemble\n",
      "\n",
      "print \"after outlier removal\"\n",
      "print \"non-pois people count: \" + str(data_frame[\"poi\"].loc[data_frame[\"poi\"]==False].count())\n",
      "print \"pois people count: \" + str(data_frame[\"poi\"].loc[data_frame[\"poi\"]==True].count())\n",
      "print \"\\n\\n\"\n",
      "df = data_frame.fillna(0.0)\n",
      "df.drop([\"full_name\", \"poi\", \"email_address\"], axis=1, inplace=True)\n",
      "X = np.array(df.values)\n",
      "Y = np.array(data_frame[\"poi\"].values)\n",
      "dt = ensemble.ExtraTreesClassifier(min_samples_split=10)\n",
      "dt.fit(X, Y)\n",
      "d = dict(zip(df.columns.values, dt.feature_importances_))\n",
      "feature_toRemove = []\n",
      "for feature in d.keys():\n",
      "    print feature + \" importance: \" + str(d[feature])\n",
      "    if d[feature] < 0.01:\n",
      "        feature_toRemove.append(feature)\n",
      "print \"feature to remove \" + str(feature_toRemove)\n",
      "data_frame.drop(feature_toRemove, axis=1, inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "after outlier removal\n",
        "non-pois people count: 109\n",
        "pois people count: 16\n",
        "\n",
        "\n",
        "\n",
        "salary importance: 0.0504682621883\n",
        "to_messages importance: 0.0428895250492\n",
        "total_payments importance: 0.0594110885179\n",
        "bonus importance: 0.0455325957014\n",
        "total_stock_value importance: 0.0920903837691\n",
        "shared_receipt_with_poi importance: 0.0387864757804\n",
        "from_poi_to_this_person importance: 0.0444329313516\n",
        "exercised_stock_options importance: 0.113989862967\n",
        "from_messages importance: 0.00232890492636\n",
        "other importance: 0.0701952216163\n",
        "from_this_person_to_poi importance: 0.0678702749747\n",
        "deferred_income importance: 0.0924885690681\n",
        "expenses importance: 0.132781671985\n",
        "restricted_stock importance: 0.0511990179652\n",
        "long_term_incentive importance: 0.0955352141402\n",
        "feature to remove ['from_messages']\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For sure I have to remove those feature for which the importance is equal or very close to zero"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "New Features creation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I have to pay attention on the final number of features I will use for my final classifier. So far I have 9 features, but the dataset has been shrinked to about 130  entries. To not run into overfitting I can not add too many features.\n",
      "I start adding some transformed features, I will add squared, log and and mixed products features, only for those features that are important. Then I'll check if they are important and removed those are not."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FEATURE_THRESHOLD = 0.1\n",
      "important_feature = []\n",
      "\n",
      "for feature in d.keys():\n",
      "    if d[feature] > FEATURE_THRESHOLD:\n",
      "        important_feature.append(feature)\n",
      "for i in range(0, len(important_feature)):\n",
      "    ifeature = important_feature[i]\n",
      "    en_feature = ifeature + str(\"^2\")\n",
      "    data_frame[en_feature] = data_frame[ifeature] **2;    \n",
      "    en_feature = \"log_\" + ifeature\n",
      "    data_frame[en_feature] = data_frame[ifeature].apply(np.log);\n",
      "    for j in range(i+1, len(important_feature)):\n",
      "        jfeature = important_feature[j]\n",
      "        en_feature = ifeature + \"*\" + jfeature\n",
      "        data_frame[en_feature] = data_frame[ifeature] * data_frame[jfeature];\n",
      "\n",
      "df = data_frame.fillna(0.0)\n",
      "df.drop([\"full_name\", \"poi\", \"email_address\"], axis=1, inplace=True)\n",
      "X = np.array(df.values)\n",
      "Y = np.array(data_frame[\"poi\"].values)\n",
      "dt = ensemble.ExtraTreesClassifier(min_samples_split=10)\n",
      "dt.fit(X, Y)\n",
      "d = dict(zip(df.columns.values, dt.feature_importances_))\n",
      "feature_toRemove = []\n",
      "for feature in d.keys():\n",
      "    print feature + \" importance: \" + str(d[feature])\n",
      "    if d[feature] < 0.01:\n",
      "        feature_toRemove.append(feature)\n",
      "print \"feature to remove \" + str(feature_toRemove)\n",
      "data_frame.drop(feature_toRemove, axis=1, inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "salary importance: 0.0426760879083\n",
        "to_messages importance: 0.000421235263987\n",
        "log_exercised_stock_options importance: 0.0609749917996\n",
        "total_payments importance: 0.0127889214331\n",
        "exercised_stock_options^2 importance: 0.0875267358265\n",
        "bonus importance: 0.0735379739075\n",
        "total_stock_value importance: 0.0445114433178\n",
        "shared_receipt_with_poi importance: 0.0285939086855\n",
        "from_poi_to_this_person importance: 0.0388802748438\n",
        "exercised_stock_options importance: 0.0808218041804\n",
        "log_expenses importance: 0.0764445872084\n",
        "other importance: 0.0486568448522\n",
        "from_this_person_to_poi importance: 0.015662799332\n",
        "expenses^2 importance: 0.0523513005977\n",
        "deferred_income importance: 0.0667137264845\n",
        "expenses importance: 0.0511525599732\n",
        "restricted_stock importance: 0.0467285456345\n",
        "long_term_incentive importance: 0.099801401944\n",
        "exercised_stock_options*expenses importance: 0.0717548568068\n",
        "feature to remove ['to_messages']\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now I start analyzing mail dataset. It counts on a huge collection of mails, I have to pay attention on computation time and memory usage.\n",
      "My idea is to find the most meaningful words subset of the total bag of words.\n",
      "First I create a class EnronEmployee to simplify mail analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "sys.path.append( \"../tools/\" )\n",
      "from parse_out_email_text import parseOutText \n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "\n",
      "class EnronEmployee(object):\n",
      "    def __init__(self, name, data_frame):\n",
      "        self.name_ = name\n",
      "        self.dframe_ = data_frame\n",
      "        if self.getEmail():\n",
      "            self.retrieveMailLists()\n",
      "            self.countEmails()\n",
      "        else:\n",
      "            self.recvdMailCount_ = 0\n",
      "            self.sentMailCount_ = 0\n",
      "            self.totalMailCount_ = 0\n",
      "        self.checkIfPoi()\n",
      "    \n",
      "    def getEmail(self):\n",
      "        self.email_ = self.dframe_[\"email_address\"].loc[self.dframe_[\"full_name\"] == self.name_].iloc[0]\n",
      "        return self.email_ is not \"\"\n",
      "    \n",
      "    def checkIfPoi(self):\n",
      "        self.isPoi_ = self.dframe_[\"poi\"].loc[self.dframe_[\"full_name\"] == self.name_].iloc[0]\n",
      "        \n",
      "    def retrieveMailLists(self):\n",
      "        tmp = \"./emails_by_address/from_\" + self.email_ + \".txt\"\n",
      "        if os.path.isfile(tmp):\n",
      "            self.fromList_ = tmp\n",
      "        else:\n",
      "            self.fromList_ = \"\"            \n",
      "        tmp = \"./emails_by_address/to_\" + self.email_ + \".txt\"\n",
      "        if os.path.isfile(tmp):\n",
      "            self.toList_ = tmp\n",
      "        else:\n",
      "            self.toList_ = \"\"\n",
      "\n",
      "    def countEmails(self):\n",
      "        self.sentMailCount_ = 0\n",
      "        self.recvMailCount_ = 0\n",
      "        if self.fromList_ is not \"\": \n",
      "            mailList = open(self.fromList_,\"r\");\n",
      "            self.sentMailCount_ = sum(1 for line in mailList)\n",
      "        else:\n",
      "            self.sentMailCount_ = 0\n",
      "        if self.toList_ is not \"\":    \n",
      "            mailList = open(self.toList_,\"r\");\n",
      "            self.recvdMailCount_ = sum(1 for line in mailList)\n",
      "        else:\n",
      "            self.recvdMailCount_ = 0\n",
      "        self.totalMailCount_ = self.sentMailCount_ + self.recvdMailCount_\n",
      "    \n",
      "    def getEmailCount(self):\n",
      "        return self.totalMailCount_\n",
      "        \n",
      "    def analyzeMails(self, limit_from = float('Infinity'), limit_to = float('Infinity')):     \n",
      "        if self.totalMailCount_ == 0:\n",
      "            return [], []\n",
      "        tmp = []\n",
      "        if self.fromList_ is not \"\":\n",
      "            if self.sentMailCount_ > limit_from:\n",
      "                sentMailIdx = np.random.choice(self.sentMailCount_, limit_from)\n",
      "            else:\n",
      "                sentMailIdx = range(0, self.sentMailCount_)\n",
      "            midx = 0\n",
      "            mailList = open(self.fromList_,\"r\");\n",
      "            for path in mailList:\n",
      "                if midx in sentMailIdx:\n",
      "                    path = os.path.join('..', path[:-1])\n",
      "                    email = open(path, \"r\")\n",
      "                    str_email = parseOutText(email)                              \n",
      "                    tmp.append(str_email)\n",
      "                midx += 1\n",
      "            word_data = tmp\n",
      "            poi_data = [int(self.isPoi_)] * len(tmp)\n",
      "            \n",
      "        word_data = tmp\n",
      "        tmp = []      \n",
      "        if self.toList_ is not \"\":\n",
      "            if self.recvdMailCount_ > limit_to:\n",
      "                recvdMailIdx = np.random.choice(self.recvdMailCount_, limit_to)\n",
      "            else:\n",
      "                recvdMailIdx = range(0, self.recvdMailCount_)\n",
      "            midx = 0\n",
      "            mailList = open(self.toList_,\"r\");\n",
      "            for path in mailList:\n",
      "                if midx in recvdMailIdx:\n",
      "                    path = os.path.join('..', path[:-1])\n",
      "                    email = open(path, \"r\")\n",
      "                    str_email = parseOutText(email)                               \n",
      "                    tmp.append(str_email)\n",
      "                midx += 1               \n",
      "            poi_data += [int(self.isPoi_)] * len(tmp)        \n",
      "            word_data += tmp\n",
      "            return word_data, poi_data\n",
      "    \n",
      "    def vectorizeMails(self, dictio):\n",
      "        words, _ = self.analyzeMails()\n",
      "        total_words = \"\"\n",
      "        for w in words:\n",
      "            total_words += \" \" + w\n",
      "        vecter = TfidfVectorizer(sublinear_tf=True, vocabulary = dictio)\n",
      "        scores = vecter.fit_transform([total_words])\n",
      "        result = {}\n",
      "        scores = scores.toarray()\n",
      "        for i in range(0, len(scores[0])):\n",
      "            result[vecter.get_feature_names()[i]] = scores[0,i]\n",
      "        return result\n",
      "            \n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I analyze mails and start to add some new features:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_frame[\"total_messages\"] = pd.Series(np.zeros(len(data_frame)))\n",
      "for name in data_frame[\"full_name\"].values:    \n",
      "    person = EnronEmployee(name, data_frame)\n",
      "    data_frame[\"total_messages\"].loc[data_frame[\"full_name\"] == name] = person.getEmailCount()\n",
      "\n",
      "print data_frame[\"total_messages\"].describe()\n",
      "print \"\\n\"\n",
      "print \"total email messages: \" + str(data_frame[\"total_messages\"].sum())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "count      125.000000\n",
        "mean      1267.944000\n",
        "std       2064.674419\n",
        "min          0.000000\n",
        "25%          0.000000\n",
        "50%        516.000000\n",
        "75%       1779.000000\n",
        "max      12587.000000\n",
        "Name: total_messages, dtype: float64\n",
        "\n",
        "\n",
        "total email messages: 158493.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/lib/python2.7/dist-packages/pandas/compat/scipy.py:68: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
        "  score = values[idx]\n",
        "/usr/lib/python2.7/dist-packages/pandas/compat/scipy.py:68: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
        "  score = values[idx]\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is impossible to process all the e-mails to find the most important words at once. I have to find a way to find these words while picking only a very small subset of the email messages.\n",
      "I decide to randomly pick 20 persons, keeping poi-nonpoi proportion, and then analyzed their first 200 hundreds mails. Take note of the most meaningful words and repeat the process 50 times adding the \"importance\" score for those words. At the end I will pick the first 10 words most important"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import ensemble\n",
      "\n",
      "ATTEMPTS = 50\n",
      "PERSON_SUBSET = 20\n",
      "MAIL_PER_PERSON = 200\n",
      "important_word_dict = {}\n",
      "poi_ratio = float(data_frame[\"poi\"].loc[data_frame[\"poi\"]==True].count()) / float(data_frame[\"poi\"].count())\n",
      "\n",
      "for i in range(0,ATTEMPTS):\n",
      "    if i % 10 == 0:\n",
      "        print \"attempt \" + str(i)\n",
      "    poi_person = data_frame.loc[data_frame[\"poi\"]==True]    \n",
      "    rnd_poi_person = poi_person.loc[np.random.choice(poi_person.index, int(round(poi_ratio*PERSON_SUBSET)))]\n",
      "    nonpoi_person = data_frame.loc[data_frame[\"poi\"]==False]    \n",
      "    rnd_nonpoi_person = nonpoi_person.loc[np.random.choice(nonpoi_person.index, int(round((1-poi_ratio)*PERSON_SUBSET)))]\n",
      "    sub_d_frame = pd.concat([rnd_poi_person, rnd_nonpoi_person])\n",
      "    sub_d_frame.reset_index(inplace = True)\n",
      "    \n",
      "    email_list = []\n",
      "    poi_labels = []\n",
      "    for name in sub_d_frame[\"full_name\"].values:\n",
      "        person = EnronEmployee(name, sub_d_frame)\n",
      "        tmp1, tmp2 = person.analyzeMails(MAIL_PER_PERSON/2, MAIL_PER_PERSON/2)\n",
      "        email_list += tmp1\n",
      "        poi_labels += tmp2\n",
      "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.25, stop_words=\"english\")\n",
      "    features_train = vectorizer.fit_transform (email_list)\n",
      "    dt = ensemble.ExtraTreesClassifier()\n",
      "    dt.fit(features_train.toarray(), poi_labels)\n",
      "  \n",
      "    importances = dt.feature_importances_\n",
      "    indices = np.argsort(importances)[::-1]    \n",
      "    sumw = 0\n",
      "    idx = 0\n",
      "\n",
      "    while sumw < 0.9 and idx < 1000:\n",
      "        word = vectorizer.get_feature_names()[indices[i]]\n",
      "        score = importances[indices[i]]\n",
      "        \n",
      "        if word in important_word_dict:\n",
      "            important_word_dict[word] += score / ATTEMPTS\n",
      "        else:\n",
      "            important_word_dict[word] = score / ATTEMPTS\n",
      "        sumw += importances[indices[i]]\n",
      "        idx += 1\n",
      "\n",
      "print important_word_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "attempt 0\n",
        "attempt 10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "attempt 20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "attempt 30"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "attempt 40"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{u'kennethpst': 0.018075982953617654, u'comment': 0.018060888504150378, u'http2072119394extravaganzahtml': 0.018078953777566777, u'eb27c2': 0.018015800544133556, u'appli': 0.018022696118177174, u'redmondhouectect': 0.01800211941973235, u'dash': 0.018021748099807968, u'advisori': 0.01844406628915676, u'shelbyenron': 0.018039000729927986, u'associateanalyst': 0.036067786677455832, u'conf': 0.01808944001039053, u'37418': 0.018064482398903667, u'jdasovicnsf': 0.018017043257766204, u'415': 0.018029110164945711, u'39610': 0.018010674063980055, u'turbin': 0.018008726194506032, u'communicationsenron': 0.0180556665585755, u'attach': 0.018057448526259037, u'jskillinnsf': 0.018061327147074126, u'remind': 0.018030524472376463, u'mhaedicnsf': 0.018112021971998918, u'adress': 0.01802721903430211, u'07212000': 0.018073815078224184, u'ken': 0.018063322500867163, u'delainey': 0.018139920399040731, u'ndas': 0.018040958728360163, u'cdwr': 0.018082533542563714, u'financ': 0.018006229479320041, u'rhayslensf': 0.01801705875275952, u'klay': 0.018058357091936917, u'0949': 0.018041117675255944, u'iniat': 0.018005081388064129, u'fax415': 0.018015539960307254, u'x35424': 0.018029510569943417, u'day': 0.018042926445719344, u'klaynsf': 0.01800999609930487, u'question': 0.018058903929683172, u'ut': 0.018011440297253472, u'gwhalleynsf': 0.036286697763749785, u'david': 0.01815502877994574, u'srs': 0.01808720801547448, u'greg': 0.018059140517157228, u'1100': 0.018014623822946982, u'13002pst': 0.018040935975983871, u'field': 0.018027610332934371, u'skeannsf': 0.018149054505414649, u'805': 0.018012631883660646, u'bond': 0.018034696626314579}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "\n",
      "pickle.dump( important_word_dict, open(\"important_word_pickle.pkl\", \"w\") )\n",
      "pickle.dump( data_frame, open(\"data_frame_1.pkl\", \"w\") )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I add the vocabulary of the first 10 most important word in the pandas data frame:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import operator\n",
      "import pickle\n",
      "\n",
      "important_word_dict = pickle.load( open(\"important_word_pickle.pkl\", \"r\"))\n",
      "data_frame = pickle.load( open(\"data_frame_1.pkl\", \"r\") )\n",
      "\n",
      "sorted_wlist = sorted(important_word_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
      "vocabulary = {}\n",
      "for i in range(0,10):\n",
      "    k = sorted_wlist[i]    \n",
      "    vocabulary[k[0]] = i\n",
      "    data_frame[k[0]] = 0.0\n",
      "\n",
      "for entry in range(0,len(data_frame)):\n",
      "    person = EnronEmployee(data_frame[\"full_name\"].iloc[entry], data_frame)\n",
      "    mails = person.vectorizeMails(vocabulary)\n",
      "    vect_words = person.vectorizeMails(vocabulary)\n",
      "    for word in vect_words.keys():\n",
      "        data_frame[word].loc[entry] = vect_words[word]\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then I will analyze the features added using the same method as before"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = data_frame.fillna(0.0)\n",
      "df.drop([\"full_name\", \"poi\", \"email_address\"], axis=1, inplace=True)\n",
      "X = np.array(df.values)\n",
      "Y = np.array(data_frame[\"poi\"].values)\n",
      "dt = ensemble.ExtraTreesClassifier(min_samples_split=10)\n",
      "dt.fit(X, Y)\n",
      "d = dict(zip(df.columns.values, dt.feature_importances_))\n",
      "feature_toRemove = []\n",
      "for feature in d.keys():\n",
      "    print feature + \" importance: \" + str(d[feature])\n",
      "    if d[feature] < 0.01:\n",
      "        feature_toRemove.append(feature)\n",
      "print \"feature to remove \" + str(feature_toRemove)\n",
      "data_frame.drop(feature_toRemove, axis=1, inplace=True)\n",
      "\n",
      "pickle.dump( data_frame, open(\"data_frame_2.pkl\", \"w\") )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "expenses importance: 0.037688113567\n",
        "advisori importance: 0.0\n",
        "log_expenses importance: 0.0867937368412\n",
        "delainey importance: 0.00386298693731\n",
        "associateanalyst importance: 0.00306347559731\n",
        "conf importance: 0.00304084873467\n",
        "deferred_income importance: 0.0718894442721\n",
        "david importance: 0.00242988058873\n",
        "from_poi_to_this_person importance: 0.0455608430645\n",
        "exercised_stock_options*expenses importance: 0.0618824607781\n",
        "log_exercised_stock_options importance: 0.0409435466719\n",
        "srs importance: 0.0\n",
        "shared_receipt_with_poi importance: 0.0608755384918\n",
        "other importance: 0.0472335361766\n",
        "mhaedicnsf importance: 0.00396124598685\n",
        "long_term_incentive importance: 0.0402750222067\n",
        "total_messages importance: 0.009711312187\n",
        "cdwr importance: 0.0110847980801\n",
        "bonus importance: 0.0528505203788\n",
        "total_stock_value importance: 0.0500076983099\n",
        "from_this_person_to_poi importance: 0.0449417437145\n",
        "expenses^2 importance: 0.0121866238277\n",
        "restricted_stock importance: 0.0372521343992\n",
        "salary importance: 0.045061117576\n",
        "total_payments importance: 0.0186956737462\n",
        "gwhalleynsf importance: 0.0141663959469\n",
        "exercised_stock_options^2 importance: 0.117972054166\n",
        "exercised_stock_options importance: 0.0683325607234\n",
        "skeannsf importance: 0.00823668702969\n",
        "feature to remove [u'advisori', u'delainey', u'associateanalyst', u'conf', u'david', u'srs', u'mhaedicnsf', 'total_messages', u'skeannsf']\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Analyzing performances of classifiers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that the dataset is ready, I start analyzing classifiers' performances. Before every classification algotrithm i want to apply PCA, since in financial dataset there is for sure, some correlation among different features, with PCA I'll transform features and, by discarding transformed features with less variability I hope I'll remove this correlation among features (not only within financial ones). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "import sys\n",
      "from tester import test_classifier, dump_classifier_and_data\n",
      "from Giulio_aux import *\n",
      "sys.path.append(\"../tools/\")\n",
      "from feature_format import featureFormat, targetFeatureSplit\n",
      "from sklearn import cross_validation\n",
      "\n",
      "data_frame = pickle.load( open(\"data_frame_2.pkl\", \"r\"))\n",
      "my_dataset = pdFrame2Dict(data_frame)\n",
      "\n",
      "tmp = list(data_frame.columns.values)\n",
      "tmp.remove(\"full_name\")\n",
      "tmp.remove(\"email_address\")\n",
      "tmp.remove(\"poi\")\n",
      "feature_list = [\"poi\"]\n",
      "feature_list += tmp\n",
      "\n",
      "matrix  = featureFormat(my_dataset, feature_list);\n",
      "labels, features = targetFeatureSplit(matrix)\n",
      "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels)\n",
      "\n",
      "\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn import ensemble\n",
      "from sklearn import tree\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn import preprocessing\n",
      "\n",
      "pca_params = {\"pca__n_components\" : [10, 12, 15, 17]}\n",
      "tree_params = {\"tree__min_samples_split\" : [2, 3, 5, 7, 10]}\n",
      "rndft_params = {\"rndft__n_estimators\" : [2, 3, 5, 7, 15, 20, 50]}\n",
      "svc_params = {\"svc__C\" : [0.5, 1, 5, 10, 100]}\n",
      "adabst_params = {\"adabst__n_estimators\" : [5, 10, 15, 20, 50]}\n",
      "\n",
      "\n",
      "\n",
      "tmpclf = Pipeline([ (\"pca\", PCA()), (\"tree\", tree.DecisionTreeClassifier())])\n",
      "tmp = pca_params.copy()\n",
      "tmp.update(tree_params)\n",
      "res = GridSearchCV(tmpclf, tmp, scoring=\"f1\")\n",
      "print \"decisionTree result\"\n",
      "res.fit(features_train, labels_train)\n",
      "res.score(features_test, labels_test)\n",
      "print res.best_score_\n",
      "BEST_SCORE = res.best_score_\n",
      "clf = Pipeline([ (\"pca\", PCA(res.best_params_[\"pca__n_components\"])), \n",
      "                (\"tree\", tree.DecisionTreeClassifier(res.best_params_[\"tree__min_samples_split\"]))])\n",
      "#test_classifier(res, my_dataset,feature_list)\n",
      "\n",
      "clf = Pipeline([(\"pca\", PCA()), \n",
      "                (\"rndft\", ensemble.RandomForestClassifier())])\n",
      "tmp = pca_params.copy()\n",
      "tmp.update(rndft_params)\n",
      "res = GridSearchCV(clf, tmp, scoring=\"f1\")\n",
      "print \"Random forest result\"\n",
      "res.fit(features_train, labels_train)\n",
      "res.score(features_test, labels_test)\n",
      "print res.best_score_\n",
      "if BEST_SCORE < res.best_score_:\n",
      "    clf = Pipeline([ (\"pca\", PCA(res.best_params_[\"pca__n_components\"])), \n",
      "                (\"rndft\", ensemble.RandomForestClassifier(res.best_params_[\"rndft__n_estimators\"]))])\n",
      "    BEST_SCORE = res.best_score_\n",
      "#test_classifier(res, my_dataset,feature_list)\n",
      "#test_classifier(res, my_dataset,feature_list)\n",
      "\n",
      "clf = Pipeline([ (\"scale\", preprocessing.MinMaxScaler()),(\"pca\", PCA()), (\"svc\", LinearSVC())])\n",
      "tmp = pca_params.copy()\n",
      "tmp.update(svc_params)\n",
      "res = GridSearchCV(clf, tmp, scoring=\"f1\")\n",
      "print \"SVC result\"\n",
      "res.fit(features_train, labels_train)\n",
      "res.score(features_test, labels_test)\n",
      "print res.best_score_\n",
      "if BEST_SCORE < res.best_score_:\n",
      "    clf = Pipeline([ (\"pca\", PCA(res.best_params_[\"pca__n_components\"])), \n",
      "                (\"svc\", LinearSVC(res.best_params_[\"svc__C\"]))])\n",
      "    BEST_SCORE = res.best_score_\n",
      "\n",
      "\n",
      "clf = Pipeline([(\"pca\", PCA()), (\"adabst\", ensemble.AdaBoostClassifier())])\n",
      "tmp = pca_params.copy()\n",
      "tmp.update(adabst_params)\n",
      "res = GridSearchCV(clf, tmp, scoring=\"f1\")\n",
      "print \"adaboost result\"\n",
      "res.fit(features_train, labels_train)\n",
      "res.score(features_test, labels_test)\n",
      "print res.best_score_\n",
      "if BEST_SCORE < res.best_score_:\n",
      "    clf = Pipeline([ (\"pca\", PCA(res.best_params_[\"pca__n_components\"])), \n",
      "                (\"adabst\", ensemble.AdaBoostClassifier(res.best_params_[\"adabst__n_estimators\"]))])\n",
      "    BEST_SCORE = res.best_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "decisionTree result\n",
        "0.302645502646"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Random forest result\n",
        "0.361111111111"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "SVC result\n",
        "0.177777777778"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "adaboost result\n",
        "0.207407407407"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So I finally test the best classifier with the tester application from the base code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = Pipeline([ (\"pca\", PCA(n_components=10)), (\"tree\", tree.DecisionTreeClassifier(min_samples_split=2))])\n",
      "test_classifier(clf, my_dataset,feature_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pipeline(pca=PCA(copy=True, n_components=10, whiten=False), pca__copy=True,\n",
        "     pca__n_components=10, pca__whiten=False,\n",
        "     tree=DecisionTreeClassifier(compute_importances=None, criterion=gini,\n",
        "            max_depth=None, max_features=None, min_density=None,\n",
        "            min_samples_leaf=1, min_samples_split=2, random_state=None,\n",
        "            splitter=best),\n",
        "     tree__compute_importances=None, tree__criterion=gini,\n",
        "     tree__max_depth=None, tree__max_features=None, tree__min_density=None,\n",
        "     tree__min_samples_leaf=1, tree__min_samples_split=2,\n",
        "     tree__random_state=None, tree__splitter=best)\n",
        "\tAccuracy: 0.79969\tPrecision: 0.33676\tRecall: 0.31150\tF1: 0.32364\tF2: 0.31624\n",
        "\tTotal predictions: 13000\tTrue positives:  623\tFalse positives: 1227\tFalse negatives: 1377\tTrue negatives: 9773\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print data_frame.loc[0]\n",
      "import pickle\n",
      "my_feature_list = [\"poi\",\"total_payments\", \"bonus\", \"expenses\", \"total_stock_value\", \"salary\",\n",
      "                   \"exercised_stock_options\", \"restricted_stock\", \"shared_receipt_with_poi\",\n",
      "                   \"other\", \"from_this_person_to_poi\", \"deferred_income\", \"long_term_incentive\",\n",
      "                   \"from_poi_to_this_person\", \"exercised_stock_options^2\", \"log_exercised_stock_options\",\n",
      "                   \"exercised_stock_options*expenses\", \"expenses^2\", \"log_expenses\", \"gwhalleynsf\",\n",
      "                   \"cdwr\"]\n",
      "\n",
      "my_dataset = pdFrame2Dict(data_frame)\n",
      "my_classifier = clf\n",
      "               \n",
      "pickle.dump(my_dataset, open(\"my_dataset.pkl\", \"w\") )\n",
      "pickle.dump(my_feature_list, open(\"my_feature_list.pkl\", \"w\") )\n",
      "pickle.dump(my_classifier, open(\"my_classifier.pkl\", \"w\") )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "full_name                                     METTS MARK\n",
        "salary                                            365788\n",
        "total_payments                                   1061827\n",
        "exercised_stock_options                              NaN\n",
        "bonus                                             600000\n",
        "restricted_stock                                  585062\n",
        "shared_receipt_with_poi                              702\n",
        "total_stock_value                                 585062\n",
        "expenses                                           94299\n",
        "other                                               1740\n",
        "from_this_person_to_poi                                1\n",
        "poi                                                False\n",
        "deferred_income                                      NaN\n",
        "long_term_incentive                                  NaN\n",
        "email_address                       mark.metts@enron.com\n",
        "from_poi_to_this_person                               38\n",
        "exercised_stock_options^2                            NaN\n",
        "log_exercised_stock_options                          NaN\n",
        "exercised_stock_options*expenses                     NaN\n",
        "expenses^2                                  8.892301e+09\n",
        "log_expenses                                    11.45423\n",
        "gwhalleynsf                                    0.4050595\n",
        "cdwr                                           0.3729847\n",
        "Name: 0, dtype: object\n"
       ]
      }
     ],
     "prompt_number": 26
    }
   ],
   "metadata": {}
  }
 ]
}